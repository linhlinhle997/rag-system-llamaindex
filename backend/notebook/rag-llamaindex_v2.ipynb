{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T04:43:06.667530Z",
     "iopub.status.busy": "2025-02-17T04:43:06.667215Z",
     "iopub.status.idle": "2025-02-17T04:43:35.243649Z",
     "shell.execute_reply": "2025-02-17T04:43:35.242845Z",
     "shell.execute_reply.started": "2025-02-17T04:43:06.667504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# For hugging face models\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# For reader\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# For index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# For loading index from storage\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# For Query\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "LLM_NAME = os.getenv(\"LLM_NAME\", \"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "EMBED_MODEL_NAME = os.getenv(\"EMBED_MODEL_NAME\", \"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !gdown 1rtntaSqlpDMkINEzbCDwsAMSkr2V5bHM\n",
    "# !unzip data.zip\n",
    "# !rm data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "STORAGE_PATH = \"storage\"\n",
    "METADATA_PATH = os.path.join(STORAGE_PATH, \"processed_files.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(DATA_PATH).load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39691/2336832613.py:1: DeprecationWarning: Call to deprecated class HuggingFaceInferenceAPI. (Deprecated in favor of `HuggingFaceInferenceAPI` from `llama-index-llms-huggingface-api` which should be used instead.)\n",
      "  llm = HuggingFaceInferenceAPI(model_name=LLM_NAME, token=HF_TOKEN)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceInferenceAPI(model_name=LLM_NAME, token=HF_TOKEN)\n",
    "embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "os.makedirs(STORAGE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_files(processed_files):\n",
    "    \"\"\"Save the list of processed files\"\"\"\n",
    "    with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_files, f)\n",
    "\n",
    "\n",
    "def load_processed_files():\n",
    "    \"\"\"Load the list of processed files\"\"\"\n",
    "    if os.path.exists(METADATA_PATH):\n",
    "        with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def load_existing_index():\n",
    "    \"\"\"Load the existing index if it exists\"\"\"\n",
    "    if os.path.exists(os.path.join(STORAGE_PATH, \"docstore.json\")):\n",
    "        print(\"Loading vector database from storage...\")\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=STORAGE_PATH)\n",
    "        index = load_index_from_storage(storage_context, embed_model=embed_model)\n",
    "\n",
    "        processed_files = load_processed_files()\n",
    "\n",
    "        print(f\"Number of previously processed files: {len(processed_files)}\")\n",
    "\n",
    "        for file_name, file_info in processed_files.items():\n",
    "            print(f\"   - {file_name} ({file_info.get('nodes_count', 0)} nodes)\")\n",
    "        return index, processed_files\n",
    "    else:\n",
    "        print(\"Vector database is empty, creating a new index!\")\n",
    "        return None, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_documents(documents, text_splitter, existing_index, processed_files):\n",
    "    \"\"\"\n",
    "    Process new documents and update metadata\n",
    "    \"\"\"\n",
    "    # Group documents by file name\n",
    "    docs_by_filename = {}\n",
    "    for doc in documents:\n",
    "        file_path = doc.metadata.get(\"file_path\", \"\")\n",
    "        file_name = (\n",
    "            os.path.basename(file_path)\n",
    "            if file_path\n",
    "            else doc.metadata.get(\"file_name\", \"unknown\")\n",
    "        )\n",
    "\n",
    "        if file_name not in docs_by_filename:\n",
    "            docs_by_filename[file_name] = []\n",
    "        docs_by_filename[file_name].append(doc)\n",
    "\n",
    "    # Process each file\n",
    "    for file_name, file_docs in docs_by_filename.items():\n",
    "        # Create nodes for the file\n",
    "        combined_text = \" \".join(doc.text for doc in file_docs)\n",
    "        file_hash = hashlib.md5(combined_text.encode()).hexdigest()\n",
    "\n",
    "        # Create nodes for the file\n",
    "        all_nodes = []\n",
    "        for doc in file_docs:\n",
    "            nodes = text_splitter.get_nodes_from_documents([doc])\n",
    "            print(f\"File {file_name}: created {len(nodes)} nodes\")\n",
    "            all_nodes.extend(nodes)\n",
    "\n",
    "            # Add nodes to the index if available\n",
    "            if existing_index:\n",
    "                existing_index.insert_nodes(nodes)\n",
    "\n",
    "        # Save processed file information\n",
    "        processed_files[file_name] = {\n",
    "            \"hash\": file_hash,\n",
    "            \"nodes_count\": len(all_nodes),\n",
    "            \"last_processed\": datetime.now().isoformat(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str, documents):\n",
    "    \"\"\"\n",
    "    Perform a RAG query using stored documents.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(STORAGE_PATH, exist_ok=True)\n",
    "\n",
    "    text_splitter = SentenceSplitter(chunk_size=2048, chunk_overlap=256)\n",
    "\n",
    "    # Load or create the index\n",
    "    index, processed_files = load_existing_index()\n",
    "\n",
    "    print(\n",
    "        f\"Number of nodes in the database BEFORE adding: {len(index.docstore.docs) if index else 0}\"\n",
    "    )\n",
    "\n",
    "    # If index does not exist, create a new one\n",
    "    if index is None:\n",
    "        print(f\"Creating a new index from {len(documents)} documents...\")\n",
    "        # Create index and save metadata\n",
    "        process_new_documents(documents, text_splitter, None, processed_files)\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents, transformations=[text_splitter], embed_model=embed_model\n",
    "        )\n",
    "        save_processed_files(processed_files)\n",
    "        index.storage_context.persist(persist_dir=STORAGE_PATH)\n",
    "    else:\n",
    "        # Group documents by actual file name\n",
    "        docs_by_file = {}\n",
    "        for doc in documents:\n",
    "            file_path = doc.metadata.get(\"file_path\", \"\")\n",
    "            if not file_path:\n",
    "                file_path = doc.metadata.get(\"file_name\", \"unknown\")\n",
    "\n",
    "            if file_path not in docs_by_file:\n",
    "                docs_by_file[file_path] = []\n",
    "            docs_by_file[file_path].append(doc)\n",
    "\n",
    "        # Check and process new or changed files\n",
    "        new_or_changed_files = {}\n",
    "        for file_path, file_docs in docs_by_file.items():\n",
    "            file_name = os.path.basename(file_path)\n",
    "\n",
    "            # Calculate hash for the combined content\n",
    "            combined_text = \" \".join(doc.text for doc in file_docs)\n",
    "            current_hash = hashlib.md5(combined_text.encode()).hexdigest()\n",
    "\n",
    "            # Detect new files\n",
    "            if file_name not in processed_files:\n",
    "                print(f\"New file detected: {file_name}\")\n",
    "                new_or_changed_files[file_path] = file_docs\n",
    "            # Detect changed files\n",
    "            elif current_hash != processed_files[file_name].get(\"hash\", \"\"):\n",
    "                print(f\"Changed file detected: {file_name}\")\n",
    "                new_or_changed_files[file_path] = file_docs\n",
    "\n",
    "        if new_or_changed_files:\n",
    "            # Process new or changed files\n",
    "            all_new_docs = []\n",
    "            for file_path, file_docs in new_or_changed_files.items():\n",
    "                all_new_docs.extend(file_docs)\n",
    "\n",
    "            print(\n",
    "                f\"Processing {len(all_new_docs)} documents from {len(new_or_changed_files)} new/changed files...\"\n",
    "            )\n",
    "\n",
    "            # Process each file and update metadata\n",
    "            process_new_documents(all_new_docs, text_splitter, index, processed_files)\n",
    "\n",
    "            # Save processed files information\n",
    "            save_processed_files(processed_files)\n",
    "            if index:\n",
    "                index.storage_context.persist(persist_dir=STORAGE_PATH)\n",
    "        else:\n",
    "            print(\"No new or changed files\")\n",
    "    print(f\"Number of nodes in the database AFTER adding: {len(index.docstore.docs)}\")\n",
    "\n",
    "    # Initialize query\n",
    "    retriever = index.as_retriever(similarity_top_k=10)\n",
    "    response_synthesizer = get_response_synthesizer(llm=llm)\n",
    "\n",
    "    class SortedRetrieverQueryEngine:\n",
    "        \"\"\"Query engine that sorts based on similarity score\"\"\"\n",
    "\n",
    "        def __init__(self, retriever, response_synthesizer):\n",
    "            self.retriever = retriever\n",
    "            self.response_synthesizer = response_synthesizer\n",
    "\n",
    "        def query(self, query):\n",
    "            similarity_cutoff, max_selected_nodes = 0.5, 8\n",
    "            nodes = [\n",
    "                node\n",
    "                for node in self.retriever.retrieve(query)\n",
    "                if node.score >= similarity_cutoff\n",
    "            ]\n",
    "            return self.response_synthesizer.synthesize(\n",
    "                query,\n",
    "                sorted(nodes, key=lambda x: x.score, reverse=True)[:max_selected_nodes],\n",
    "            )\n",
    "\n",
    "    return SortedRetrieverQueryEngine(retriever, response_synthesizer).query(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database is empty, creating a new index!\n",
      "Number of nodes in the database BEFORE adding: 0\n",
      "Creating a new index from 503 documents...\n",
      "File ref (1).pdf: created 2 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 2 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 2 nodes\n",
      "File ref (1).pdf: created 2 nodes\n",
      "File ref (1).pdf: created 2 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 2 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (1).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (10).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (11).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (12).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 2 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (13).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (14).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (15).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (16).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (17).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (18).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (19).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 2 nodes\n",
      "File ref (2).pdf: created 2 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (2).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (20).pdf: created 2 nodes\n",
      "File ref (20).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (3).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 2 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (4).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (5).pdf: created 2 nodes\n",
      "File ref (5).pdf: created 2 nodes\n",
      "File ref (5).pdf: created 2 nodes\n",
      "File ref (5).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 2 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (6).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 2 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (7).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 3 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 2 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 2 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (8).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "File ref (9).pdf: created 1 nodes\n",
      "Number of nodes in the database AFTER adding: 523\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(DATA_PATH).load_data()\n",
    "\n",
    "query_text = \"Abtract of DeepSeek-VL\"\n",
    "rag_result = query_rag(query_text, documents)\n",
    "\n",
    "response = {\n",
    "    \"answer\": rag_result.response,\n",
    "    \"sources\": [\n",
    "        {\n",
    "            \"text\": node.text,\n",
    "            \"score\": node.score,\n",
    "            \"source\": node.metadata.get(\"source\", \"Unknown\"),\n",
    "        }\n",
    "        for node in rag_result.source_nodes\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '\\nDeepSeek-VL is an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. It is structured around three key dimensions: Data Construction, Model Architecture, and Training Strategy. The Data Construction focuses on ensuring diversity, scalability, and comprehensive coverage of real-world scenarios. The Model Architecture incorporates a hybrid vision encoder for handling high-resolution images within a fixed token budget. The Training Strategy aims to preserve strong language abilities during pretraining by integrating LLM training and managing the competitive dynamics between vision and language modalities.\\nThe visual module is designed to optimize the utilization of high-resolution visual inputs while remaining within a fixed token budget to manage inference costs effectively. The hybrid vision encoder combines a text-aligned encoder for coarse semantic extraction at 384 ×384 resolution with a high-resolution encoder that captures detailed visual information at 1024 ×1024 resolution.\\nDuring pretraining, maintaining a significant proportion of language data, specifically at least 70%, is essential to preserve the integrity of language knowledge within the model. The modality warm-up strategy carefully',\n",
       " 'sources': [{'text': 'DeepSeek-VL 1B DeepSeek-VL-7B\\nVision Encoder SigLIP SigLIP+SAM\\nHyperparameters Stage 1 Stage 2 Stage 3 Stage 1 Stage 2 Stage 3\\nLearning rate 1.0 ×10−3 3 ×10−5 2.0 ×10−5 1.0 ×10−3 4.2 ×10−5 2.0 ×10−5\\nLR scheduler Cosine Step Cosine Cosine Step Cosine\\nWeight decay 0.0 0.0 0.0 0.0 0.0 0.0\\nGradient clip 1.0 1.0 1.0 1.0 1.0 1.0\\nOptimizer AdamW(𝛽1 = 0.9, 𝛽2 = 0.95) AdamW( 𝛽1 = 0.9, 𝛽2 = 0.95)\\nWarm-up steps 128 2000 256 128 2000 256\\nTraining steps 15000 96000 10000 15000 42000 10000\\nBatch size 256 1024 256 256 2304 256\\nSequence length 512 4096 4096 512 4096 4096\\nSequence packing × ✓ × × ✓ ×\\nPipeline parallelism × × × × ✓ ✓\\nTable 4 |Detailed hyperparameters of our DeepSeek-VL.\\n4. Evaluation\\n4.1. Public Multimodal Benchmarks Evaluation\\nWe evaluate our models on a series of public benchmarks:\\nMultimodal comprehensive understanding datasets: MMMU (Yue et al., 2023), CM-\\nMMU (Zhang et al., 2024), MMBench (Liu et al., 2023a), MMBench-CN (Liu et al., 2023a),\\nSeedBench (Li et al., 2023a) and MMV (Yu et al., 2023b). We compare DeepSeek-VL with\\ncompetitors on MMB/MMC-dev as current official test download link is no longer active.\\nChart/table understanding datasets: OCRBench (Liu et al., 2023b);\\nHallucination datasets: POPE (Li et al., 2023b);\\nScientific problem datasets: ScienceQA (Lu et al., 2022a) and MathVista (Lu et al., 2023).\\nWe apply generation-based evaluation with greedy decoding. The generation-based evalua-\\ntion here refers to letting the model generate free texts and parsing results from generated texts.\\nThe comparative results, as illustrated in Table 5, show that DeepSeek-VL-7B surpasses most\\nopen-source models of similar size across a wide range of benchmarks.\\nDeepSeek-VL outperforms open-source models of similar size in benchmarks such as MMB,\\nMMC, and SEEDbench, even approaching proprietary models (DeepSeek-VL vs. GPT-4V = 70.4\\nvs. 71.6 on seedbench), demonstrating its powerful natural image comprehension capability. The\\nmodel also surpasses all open-source models in mathematical logic, but still lags significantly\\nbehind proprietary models like GPT-4V (36.1 vs. 47.8 on MathVista). This difference could be\\nattributed to the variance in base model sizes.\\nFurthermore, as shown in Table 6, DeepSeek-VL-1.3B significantly outperforms models\\nof comparable size. It demonstrates superior performance compared to leading open-source\\nmodels in the MMB benchmark test, while utilizing only close to half the parameters (1.3B vs.\\n2.7B), indicating its robust natural image comprehension capability. DeepSeek-VL-1.3B even\\nachieves comparable results to 7B open-source models on MathVista, further validating the\\npowerful logical understanding capabilities of the DeepSeek-VL family.\\n16',\n",
       "   'score': 0.7882174936969802,\n",
       "   'source': 'Unknown'},\n",
       "  {'text': 'DeepSeek-VL: Towards Real-World Vision-Language\\nUnderstanding\\nHaoyu Lu*1†, Wen Liu*1, Bo Zhang*1‡, Bingxuan Wang1†, Kai Dong1, Bo Liu1†, Jingxiang Sun1†,\\nTongzheng Ren1†, Zhuoshu Li1, Hao Yang1†, Yaofeng Sun1, Chengqi Deng1, Hanwei Xu1, Zhenda Xie1,\\nChong Ruan1\\n1DeepSeek-AI\\n{neal, liuwen, bo}@deepseek.com\\nhttps://github.com/deepseek-ai/DeepSeek-VL\\nAbstract\\nWe present DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world\\nvision and language understanding applications. Our approach is structured around three key\\ndimensions:\\n• Data Construction: We strive to ensure our data is diverse, scalable and extensively covers\\nreal-world scenarios including web screenshots, PDFs, OCR, charts, and knowledge-based\\ncontent (expert knowledge, textbooks), aiming for a comprehensive representation of practical\\ncontexts. Further, we create a use case taxonomy from real user scenarios and construct an\\ninstruction-tuning dataset accordingly. The fine-tuning with this dataset substantially improves\\nthe model’s user experience in practical applications.\\n• Model Architecture: Considering efficiency and the demands of most real-world scenarios,\\nDeepSeek-VL incorporates a hybrid vision encoder that efficiently processes high-resolution\\nimages (1024 x 1024) within a fixed token budget, while maintaining a relatively low computa-\\ntional overhead. This design choice ensures the model’s ability to capture critical semantic and\\ndetailed information across various visual tasks.\\n• Training Strategy: We posit that a proficient Vision-Language Model should, foremost,\\npossess strong language abilities. To ensure the preservation of LLM capabilities during\\npretraining, we investigate an effective VL pretraining strategy by integrating LLM training\\nfrom the beginning and carefully managing the competitive dynamics observed between vision\\nand language modalities. Starting with a focus on text, we gradually adjust the ratio to facilitate\\na balanced integration of both modalities.\\nThe DeepSeek-VL family (both 1.3B and 7B models) showcases superior user experiences as a\\nvision-language chatbot in real-world applications, achieving state-of-the-art or competitive\\nperformance across a wide range of visual-language benchmarks at the same model size while\\nmaintaining robust performance on language-centric benchmarks. We have made both 1.3B\\nand 7B models publicly accessible to foster innovations based on this foundation model.\\n∗Equal contribution.\\n†Work done during the internship at DeepSeek-AI.\\n‡Project lead.\\narXiv:2403.05525v2  [cs.AI]  11 Mar 2024',\n",
       "   'score': 0.7820880005020544,\n",
       "   'source': 'Unknown'},\n",
       "  {'text': 'Version DeepSeek-VL DeepSeek-VL DeepSeek-LLM\\n1B Chat 7B Chat 7B Chat\\nEncoder SigLIP SigLIP+SAM None\\nBenchmark\\nHellaSwag 56.0 68.4 68.5\\nMMLU 32.5 52.4 49.4\\nGSM8K 18.0 55.0 63.0\\nMBPP 10.0 35.2 35.2\\nAGIEval 14.0 27.8 19.3\\nTable 7 |The performance on language benchmarks.\\nmodel prediction. Perplexity-based evaluation helps to distinguish subtle probability difference\\nbetween model predictions and avoids discontinuity of exact match style evaluation. We apply\\ngeneration-based evaluation with greedy decoding for GSM8K and AGIEval. The generation-\\nbased evaluation here refers to letting the model generate free texts and parsing results from\\ngenerated texts. We apply language-modeling-based evaluation for Pile-test, which means\\ncalculating the bits-per-byte on the test corpus. And the results are illustrated in Table 7\\nIt can be observed that across the majority of language benchmarks, DeepSeek-VL performs\\ncomparably to, or even surpasses, DeepSeek-7B. For instance, it achieves scores of 68.4 vs. 68.5\\non HellaSwag, which serves as a general benchmark for evaluating general language ability.\\nDeepSeek-VL outperforms DeepSeek-7B on metrics such as MMLU and AGIEval, indicating that\\nmultimodal training methods may even aid in language tasks. Nevertheless, DeepSeek-VL-7B\\nshows a certain degree of decline in mathematics (GSM8K), which suggests that despite efforts\\nto promote harmony between vision and language modalities, there still exists a competitive\\nrelationship between them. This could be attributed to the limited model capacity (7B), and\\nlarger models might alleviate this issue significantly. Overall, DeepSeek-VL strives to achieve\\nthe goal of minimizing declines in language capability while addressing these challenges.\\n4.3. Human Evaluation\\nTo further explore the capabilities of our DeepSeek-VL, we independently construct a dataset\\nfor manual evaluation. This dataset comprises 100 questions, divided into seven categories,\\neach encompassing specific tasks. These categories and tasks are same as our taxonomy for\\nthe in-house SFT data, as shown in Table 3. This approach ensures that the tasks we test are\\nuniversal and encompass the majority of use cases for multimodal models.\\nMoreover, based on the categories and tasks described in existing reports, we collect similar\\nimage materials and developed prompts. The sources for these image materials include royalty-\\nfree image communities and photographs taken by the researchers. This methodical collection\\nand prompt formulation process ensures our dataset is both comprehensive and representative\\nof real-world multimodal model applications.\\nWe compare our DeepSeek-VL-7B with InternLM-XComposer2-VL, CogVLM and GPT-\\n4V as shown in Figure 6 (and we also provide visualization results in Appendix A). GPT-4V\\ndemonstrates exceptional performance across most dimensions. All open-source models are still\\nfar behind GPT-4V in logical reasoning, highlighting the necessity of scaling up the size of Large\\nLanguage Models (LLMs). DeepSeek-VL-7B achieves better results in overall performance,\\nreaching outcomes close to GPT-4V in Recognition, Conversion, and Commonsense Reasoning.\\n18',\n",
       "   'score': 0.7756985208597774,\n",
       "   'source': 'Unknown'},\n",
       "  {'text': 'high-resolution images without losing sight of semantic richness.\\nThe incorporation of a hybrid vision encoder, capable of handling 1024 x 1024 images within\\na constrained token budget, underscores our commitment to preserving the nuanced details\\nand semantic integrity across diverse tasks. As a result, DeepSeek-VL emerges as a pioneering\\nmodel that not only meets but exceeds the standards set by generalist models in its class. It\\nshowcases exceptional performance across a wide range of visually-centric benchmarks while\\nsustaining formidable proficiency in language-centric evaluations.\\nIn making DeepSeek-VL publicly available, we aim to catalyze further innovation and\\nexploration within the research community, providing a robust foundation upon which future\\nstudies can build. This gesture of openness is intended to facilitate the collective advancement\\nof our understanding and capabilities in handling multimodal data.\\nLooking ahead, we are excited to announce plans to scale up DeepSeek-VL to larger sizes,\\nincorporating Mixture of Experts (MoE) technology. This forthcoming expansion promises to\\nfurther enhance the model’s efficiency and effectiveness, opening up new horizons for research\\nand application in the field of AI.\\nReferences\\n01-ai. Yi-34B vision language model. https://huggingface.co/01-ai/Yi-VL-34B, 2024.\\nAbi. Screenshot to code. https://github.com/abi/screenshot-to-code, 2024.\\nAnna’s Archive. Anna’s archive. https://annas-archive.org/, 2024.\\nAnthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/introd\\nucing-claude.\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\\n2021.\\nJ. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P . Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A\\nversatile vision-language model for understanding, localization, text reading, and beyond.\\narXiv preprint arXiv:2308.12966, 2023.\\nR. Bavishi, E. Elsen, C. Hawthorne, M. Nye, A. Odena, A. Somani, and S. Ta¸ sırlar. Introducing\\nour multimodal models, 2023. URL https://www.adept.ai/blog/fuyu-8b.\\nL. Blecher. Latex-ocr. GitHub repository, 2024. URL https://github.com/lukas-blecher\\n/LaTeX-OCR.\\nL. Blecher, G. Cucurull, T. Scialom, and R. Stojnic. Nougat: Neural optical understanding for\\nacademic documents. arXiv preprint arXiv:2308.13418, 2023.\\nA. Burns, K. Srinivasan, J. Ainslie, G. Brown, B. A. Plummer, K. Saenko, J. Ni, and M. Guo. A\\nsuite of generative tasks for multi-level multimodal webpage understanding. In The 2023\\nConference on Empirical Methods in Natural Language Processing (EMNLP), 2023. URL\\nhttps://openreview.net/forum?id=rwcLHjtUmn.\\nJ. Carter. Textocr-gpt4v. https://huggingface.co/datasets/jimmycarter/textocr-g\\npt4v, 2024.\\n23',\n",
       "   'score': 0.7693707553565238,\n",
       "   'source': 'Unknown'},\n",
       "  {'text': 'DeepSeek-VL’s pretraining dataset is compiled from a variety of sources, including but not\\nlimited to Common Crawl, Web Code, E-books, Educational Materials, and arXiv Articles. This\\ncollection thoroughly encompasses real-world scenarios such as web screenshots, PDFs, OCR,\\ncharts, and knowledge-based content (expertise, textbooks), aiming for a broad and practical\\nrepresentation while remaining scalable.\\nWhile our pretraining data encompasses a wide array of world knowledge, we meticulously\\ncurate our instruction-tuning dataset to reflect real-world usage scenarios. To achieve this, we\\nmanually gather authentic test cases for GPT-4V and Gemini from the Internet. These cases have\\nbeen systematically organized into a comprehensive taxonomy. We use this structured taxonomy\\nto choose prompts for each test image, ensuring a practical and relevant instruction tuning\\ndataset. This taxonomy is also used to create an evaluation dataset that effectively assesses\\nreal-world performance.\\nThe visual module is designed to optimize the utilization of high-resolution visual inputs\\nwhile remaining within a fixed token budget to manage inference costs effectively. As such, we\\nemploy a hybrid vision encoder, which combines a text-aligned encoder for coarse semantic\\nextraction at 384 ×384 resolution with a high-resolution encoder that captures detailed visual\\ninformation at 1024 ×1024 resolution. By fusing these two encoders, our hybrid approach\\nefficiently condenses a 1024×1024 resolution image (which suffices in most use cases) into 576\\ntokens. This token count strikes a balance between rich visual representation and token economy,\\nmaking it feasible for both text-image interleaving and multi-turn inference scenarios.\\nDuring the pretraining of multimodal models, a common challenge encountered is the\\npotential degradation of language capabilities when the training process is overly reliant on\\nvision-language data. Our research reveals that maintaining a significant proportion of language\\ndata—specifically, at least 70%—is essential to preserve the integrity of language knowledge\\nwithin the model. This balance is critical for achieving a robust multimodal capability that does\\nnot compromise language performance. Moreover, we introduce a novel “modality warm-up”\\nstrategy. This approach carefully adjusts the ratio of modalities during training, gradually\\nincorporating more vision-language data. The careful tuning of the modality ratio along with\\nthe warm-up strategy results in a balanced performance of both modalities.\\nWhen iterating on our model, We conduct experiments on a small scale before scaling to a\\nlarger model size. However, a smaller model, e.g., 1B model, cannot demonstrate reasonable\\nperformance on benchmarks (Schaeffer et al., 2024) and faithfully reflect the model’s performance.\\nWe adopt two approaches to address this. First, we modify the evaluation protocol from multi-\\nchoice to compare the perplexity of options. Also, to prevent the instruction following ability\\nfrom becoming the bottleneck, we mix a small proportion of instruction tuning data during the\\npretraining phase. In this way, we can achieve reasonable performance using the 1B model and\\nmore accurately measure the impact of each iteration during the experiment.\\nThrough extensive evaluations of general vision and language benchmarks, the DeepSeek-VL\\nfamily showcases superior user experiences in real-world applications and achieves state-of-\\nthe-art or competitive performance across a wide range of visual-language benchmarks at the\\nsame model size, while maintaining robust language-centric performance. To foster innovation\\nand enable a wide range of applications, we have made two versions of our ours, 1.3B and 7B,\\npublicly accessible, in the hope of facilitating the needs of varying computational capabilities.\\n5',\n",
       "   'score': 0.7636712073996813,\n",
       "   'source': 'Unknown'},\n",
       "  {'text': '4.55\\n5.71\\n4.09\\n4.21\\n5.22\\n3.21\\n3.75\\n3.75\\n4.65\\n4.76\\n4.09\\n5.26\\n6.96\\n1.43\\n3.75\\n2.5\\n5.65\\n7.01\\n6.82\\n4.74\\n6.52\\n4.29\\n3.13\\n3.75\\n6.3\\n7.14\\n7.73\\n4.47\\n6.74\\n5.36\\n8.13\\n3.75\\nTOTAL SCORERECOGNITIONCONVERSIONANALYSISCOMMONSENSELOGICALMULTI-IMAGESEVALUATION\\nInternLM-XComposer2-VLCogVLM-17BDeepSeek-VL-7BGPT4V\\nFigure 6 | Human evaluation results on InternLM-XComposer2-VL (Dong et al., 2024),\\nCogVLM (Wang et al., 2023b), DeepSeek-VL and GPT-4V (OpenAI, 2023b).\\nFigure 7 |GPT-4V-based Evaluation Results of DeepSeek-VL vs. Other Models: The chart depicts\\nresults from a GPT-4V-based assessment across 99 test samples, demonstrating DeepSeek-VL’s\\nfavorable outcomes against both open-source and proprietary models.\\nIn addition, we conduct a comparative assessment using GPT-4V to evaluate the performance\\nof DeepSeek-VL against other models across a set of 99 test samples designed for human\\nevaluation. Following (Zheng et al., 2024), we show GPT-4V the question and the answers\\nfrom two different models and ask GPT-4V to determine which one is better or declare a\\ntie. The results indicate a preference for DeepSeek-VL’s responses in the majority of cases, as\\nGPT-4V tends to rate the quality of DeepSeek-VL’s answers more favorably. As illustrated in\\nFigure 7, DeepSeek-VL is judged to be superior in over 60% of instances when compared to open-\\nsource multimodal models, including Fuyu-8B, CogVLM-17B, and InternLM-XComposer2-VL.\\nMoreover, in comparison with other proprietary models, such as GPT-4V itself, DeepSeek-VL\\ndemonstrates comparably exceptional performance.\\n4.4. Ablation Study\\nScale Up Projector Training We expand the dataset for stage 1 (projector warmup) and sub-\\nsequently apply supervised fine-tuning. The results, depicted in Figure 8, demonstrate that\\naugmenting the training data volume does not enhance performance at this stage. This implies\\n19',\n",
       "   'score': 0.755800513530921,\n",
       "   'source': 'Unknown'},\n",
       "  {'text': 'PromptConvert it to markdown table.\\nResponse –Open-source Model 1\\nResponse –Open-source Model 2\\nResponse –DeepSeek-VLHere is the image content converted into a markdown table:\\nPlease note that the \"A\" column in the original table seems to be empty or not fully visible in the image provided.\\nFigure 14 |Visualization results. DeepSeek-VL is capable of accurately reading the contents of\\nreal-world tables.\\n33',\n",
       "   'score': 0.7539520008788725,\n",
       "   'source': 'Unknown'},\n",
       "  {'text': 'Y . Xiong, Y . Zhao, Y . He, Y . Tang, Y . Piao, Y . Dong, Y . Tan, Y . Liu, Y . Wang, Y . Guo, Y . Zhu, Y . Wang,\\nY . Zou, Y . Zha, Y . Ma, Y . Yan, Y . You, Y . Liu, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Huang, Z. Zhang, Z. Xie,\\nZ. Hao, Z. Shao, Z. Wen, Z. Xu, Z. Zhang, Z. Li, Z. Wang, Z. Gu, Z. Li, and Z. Xie (2024). Deepseek-v2:\\nA strong, economical, and efficient mixture-of-experts language model.\\nDehghani, M., J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos,\\nI. Alabdulmohsin, R. Jenatton, L. Beyer, M. Tschannen, A. Arnab, X. Wang, C. Riquelme Ruiz, M. Min-\\nderer, J. Puigcerver, U. Evci, M. Kumar, S. V . Steenkiste, G. F. Elsayed, A. Mahendran, F. Yu, A. Oliver,\\nF. Huot, J. Bastings, M. Collier, A. A. Gritsenko, V . Birodkar, C. N. Vasconcelos, Y . Tay, T. Mensink,\\nA. Kolesnikov, F. Pavetic, D. Tran, T. Kipf, M. Lucic, X. Zhai, D. Keysers, J. J. Harmsen, and N. Houlsby\\n(2023, 23–29 Jul). Scaling vision transformers to 22 billion parameters. In A. Krause, E. Brunskill, K. Cho,\\nB. Engelhardt, S. Sabato, and J. Scarlett (Eds.), Proceedings of the 40th International Conference on\\nMachine Learning, V olume 202 ofProceedings of Machine Learning Research, pp. 7480–7512. PMLR.\\nDehghani, M., B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, A. P. Steiner, J. Puigcerver,\\nR. Geirhos, I. Alabdulmohsin, A. Oliver, P. Padlewski, A. A. Gritsenko, M. Lucic, and N. Houlsby (2023).\\nPatch n’ pack: Navit, a vision transformer for any aspect ratio and resolution. In Thirty-seventh Conference\\non Neural Information Processing Systems.\\nDeng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei (2009). Imagenet: A large-scale hierarchical\\nimage database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255.\\nDesai, K., G. Kaul, Z. Aysola, and J. Johnson (2021). Redcaps: Web-curated image-text data created by\\nthe people, for the people. In J. Vanschoren and S. Yeung (Eds.),Proceedings of the Neural Information\\nProcessing Systems Track on Datasets and Benchmarks, V olume 1. Curran.\\nDong, X., P. Zhang, Y . Zang, Y . Cao, B. Wang, L. Ouyang, S. Zhang, H. Duan, W. Zhang, Y . Li, et al. (2024).\\nInternlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336\\npixels to 4k hd. arXiv preprint arXiv:2404.06512.\\nDriess, D., F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,\\nT. Yu, W. Huang, Y . Chebotar, P. Sermanet, D. Duckworth, S. Levine, V . Vanhoucke, K. Hausman,\\nM. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence (2023). Palm-e: an embodied multimodal\\nlanguage model. In Proceedings of the 40th International Conference on Machine Learning, ICML’23.\\nJMLR.org.\\nDuan, H., J. Yang, Y . Qiao, X. Fang, L. Chen, Y . Liu, X. Dong, Y . Zang, P. Zhang, J. Wang, et al.\\n(2024). Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. arXiv preprint\\narXiv:2407.11691.\\nDubey, A., A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang,\\nA. Fan, et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783.\\nGadre, S. Y ., G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh,\\nJ. Zhang, et al. (2024). Datacomp: In search of the next generation of multimodal datasets. Advances in\\nNeural Information Processing Systems 36.\\nGao, J., R. Pi, J. Zhang, J. Ye, W. Zhong, Y . Wang, L. Hong, J. Han, H. Xu, Z. Li, et al. (2023). G-llava:\\nSolving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370.\\nGao, P., R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, et al. (2024).\\nSphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint\\narXiv:2402.05935.\\n22',\n",
       "   'score': 0.7501829661230548,\n",
       "   'source': 'Unknown'}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
